# Daniel Lee LLM Questions

What ğ—Ÿğ—Ÿğ—  ğ—°ğ—¼ğ—»ğ—°ğ—²ğ—½ğ˜ğ˜€ are often asked in ğ—Ÿğ—Ÿğ— /ğ— ğ—Ÿ ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ ğ—¶ğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„?

Hereâ€™s a list asked in Google, Nvidia, Meta, Intuit and Adobe are â†“

ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² & ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´
â€¢ Transformer Architecture (attention mechanisms
â€¢ Pre-training vs Fine-tuning
â€¢ Training Objectives (next token prediction)
â€¢ Context Window and Position Embeddings
â€¢ Tokenization Strategies
â€¢ Model Scaling Laws
â€¢ Parameter Efficient Fine-tuning (LoRA, QLoRA, Prefix Tuning)

ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—–ğ—¼ğ—»ğ˜ğ—¿ğ—¼ğ—¹
â€¢ Temperature and Top-p Sampling
â€¢ Prompt Engineering Techniques
â€¢ Few-shot Learning
â€¢ In-context Learning
â€¢ Chain-of-Thought Prompting
â€¢ Hallucination Prevention

ğ—Ÿğ—Ÿğ—  ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—»
â€¢ Perplexity
â€¢ ROUGE Scores
â€¢ BLEU Scores
â€¢ Human Evaluation Methods
â€¢ Benchmark Datasets (MMLU, BigBench, HumanEval)
â€¢ Bias Detection

ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—» & ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜
â€¢ Quantization Techniques (4-bit, 8-bit)
â€¢ Model Distillation
â€¢ Prompt Caching
â€¢ Model Merging
â€¢ Inference Optimization
â€¢ Load Balancing
â€¢ Latency Management
â€¢ Cost Optimization

ğ—¦ğ—®ğ—³ğ—²ğ˜ğ˜† & ğ—˜ğ˜ğ—µğ—¶ğ—°ğ˜€
â€¢ Content Filtering
â€¢ Output Sanitization
â€¢ Jailbreak Prevention
â€¢ Data Privacy

Other areas covered in LLM Engineer or ML Engineer, Gen AI roles include system design, coding and ML depth.

â¤· What's another concept asked in LLM interviews? Drop one â†“
Ace upcoming interviews with theseğŸ‘‡

ğŸ“• ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—£ğ—¿ğ—²ğ—½ ğ—–ğ—¼ğ˜‚ğ—¿ğ˜€ğ—²ğ˜€: datainterview.com/courses
ğŸ“˜ ğ—ğ—¼ğ—¶ğ—» ğ——ğ—¦ ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—•ğ—¼ğ—¼ğ˜ğ—°ğ—®ğ—ºğ—½: https://lnkd.in/egCcmuCr
ğŸ“™ ğ—ğ—¼ğ—¶ğ—» ğ— ğ—Ÿğ—˜ ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—•ğ—¼ğ—¼ğ˜ğ—°ğ—®ğ—ºğ—½: https://lnkd.in/e5VaYyTz
